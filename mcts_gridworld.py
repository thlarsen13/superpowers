# -*- coding: utf-8 -*-
"""MCTS gridworld.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y-csjWpS4ocr5vyYNNn9VcPVflgE9xXK

"""
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import logging
logging.getLogger('tensorflow').disabled = True

from typing_extensions import ParamSpecKwargs
import numpy as np
import copy
import sys
from tqdm import tqdm
import inspect
import argparse
import tensorflow as tf 
import keras 
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
import json
from maze_generator import generate_maze
from joblib import Parallel, delayed
import math 


verbose = 4

class Environment(): 
    def __init__(self, seed=0, size=5, total_moves = 10, factor=1, random=False):
        # np.random.seed(seed)
        self.generate_size = size
        if factor > 1: 
            self.size = size * factor
        else: 
            self.size = size
        self.factor=factor
        # self.history = [self.observation()]   

        self.total_moves = total_moves #number of moves in a game
        
        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0), (0, 0)]
        self.action_size = len(self.actions) #at each timestep, what is the possible action size? (here, 5: up down left right null)

        self.random = random 

    def init_state(self): 
        state = {
            'timestep' : 0 
            # 'visited' : np.zeros((size, size), dtype=int)
        }
        self.generate_walls(state)

        if self.factor > 1: 
            self.expand(state)
        return state 

    def expand(self, state): 
        # new_size = self.size * 3 
        f = self.factor
        new_walls = np.zeros((self.size, self.size))
        for i in range(self.generate_size): 
            for j in range(self.generate_size): 
                if state['walls'][i][j] > 0: 

                    shape = np.array([[1, 0, 0], [1, 0, 0], [1, 1, 1]], dtype=int)
                    new_walls[f*i:f*(i+1), f*j:f*(j+1)] = shape

        state['walls'] = new_walls
        state['agent_xy'] = tuple([f*x for x in state['agent_xy']])
        state['coin_xy'] = tuple([f*x for x in state['coin_xy']])

    def is_valid_idx(self, x, y): 
        return 0 <= x and x < self.size and 0 <= y and y < self.size
    
    def is_valid_placement(self, idx, maze): 
        x, y = idx 
        for mv in self.actions[:-1]: 
            dx, dy = mv 
            if self.is_valid_idx(x+dx, y+dy) and maze[x+dx][y+dy] == 0: 
                return True 
        return False

    def generate_walls(self, state):

        sz = self.generate_size

        maze = generate_maze(sz)
        if not self.random: 
            for i in range(0, sz):
              if (maze[1][i] == 0):
                maze[0][i] = 0
                state['agent_xy'] = (0, i)
                # print(f'agent_xy: {(0, i)}')
                break

            for i in range(sz-1, 0, -1):
              if (maze[sz-2][i] == 0):
                maze[sz-1][i] = 0
                state['coin_xy'] = (sz-1, i)
                break
        else:
            for thing in ['coin_xy', 'agent_xy']:
                while True: 
                    Placement = tuple(np.random.choice(list(range(self.size)),(2,)))
                    if self.is_valid_placement(Placement, maze):
                        state[thing]=tuple(int(x) for x in Placement)
                        break

        state['walls'] = maze

    def make_move(self, s, a):
        new_s = copy.deepcopy(s)
        dx, dy = a 
        x, y = new_s['agent_xy']
        valid_moves = self.get_valid_moves(s) 

        if a in valid_moves:
            new_s['agent_xy'] = (x + dx, y + dy)
            new_s['timestep'] += 1
        else: 
            print('oops, invalid move suggested')
            exit()
        
        return new_s 
  
    def get_valid_moves(self, s=None): 
        if s == None: #returns all of the valid moves if not given a specific state. 
            return self.actions
    
        valid_moves = []
        for action in self.actions:
          dx, dy = action 
          x, y = s['agent_xy']
              
          valid = True 
          # if x+dx == s['coin_xy'][0] and y+dy == s['coin_xy'][1]:
          #     valid = False
          if (x+dx < 0) or (x+dx >= self.size) or (y+dy < 0) or (y+dy >= self.size):
              valid = False
          elif s['walls'][x+dx][y+dy] == 1: 
              valid = False
          if valid:
              valid_moves.append(action)
        return valid_moves

    def observation(self, s):
        obs = np.zeros((self.size,self.size, 4))

        obs[tuple(s['coin_xy']), 0] = 1 
        obs[tuple(s['agent_xy']), 1] = 1
        obs[:, 2] = s['timestep']
        obs[:, :, 3] = np.array(s['walls'])

        return obs

    def print_state(self, s): 
        # state = np.zeros((self.size,self.size))
        state = s['walls'].tolist()
        print('---state---')
        for i in range(0, self.size):
          for j in range(0, self.size):
            if (i, j) == s['coin_xy']: 
                print('C', end="")
            elif (i, j) == s['agent_xy']: 
                print('A', end="")
            elif state[i][j] > 0: 
                print(1, end="")
            else: 
                print(" ", end="")
          print('\n', end="")

    def reward(self, s): 
        a_x, a_y = s['agent_xy']
        c_x, c_y = s['coin_xy']
        d = ( (a_x - c_x) ** 2 + (a_y - c_y) ** 2 ) ** .5

        return d <= 1
        # return (self.size-d) / self.size
        return np.e **(-d)
    def done(self, s): 
        return s['timestep'] >= self.total_moves

class NNet():
    def __init__(self, env):
        # game params
        self.action_size = env.action_size
        self.num_channels = 512 
        self.batch_size = 16 
        self.epochs = 25
        self.lr = 10**-3
        self.dropout = .5

        self.in_channels = 4

        # Neural Net
        self.input_boards = Input(shape=(env.size, env.size, self.in_channels))    # s: batch_size x board_y x board_z

        # x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1
        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(self.num_channels, 3, padding='same', use_bias=False)(self.input_boards)))         # batch_size  x board_x x board_y x num_channels
        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(self.num_channels, 3, padding='same', use_bias=False)(h_conv1)))         # batch_size  x board_x x board_y x num_channels
        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(self.num_channels, 3, padding='valid', use_bias=False)(h_conv2)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels
        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(self.num_channels, 3, padding='valid', use_bias=False)(h_conv3)))        # batch_size  x (board_x-4) x (board_y-4) x num_channels
        h_conv4_flat = Flatten()(h_conv4)       
        s_fc1 = Dropout(self.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024, use_bias=False)(h_conv4_flat))))  # batch_size x 1024
        s_fc2 = Dropout(self.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512, use_bias=False)(s_fc1))))          # batch_size x 1024
        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size
        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1

        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])
        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(self.lr))

    def train(self, examples):
        """
        examples: list of examples, each example is of form (board, pi, v)
        """
        input_boards, target_pis, target_vs = list(zip(*examples))
        input_boards = np.asarray(input_boards)
        target_pis = np.asarray(target_pis)
        target_vs = np.asarray(target_vs)
        self.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = self.batch_size, epochs = self.epochs)

    def predict(self, board):

        # print(board.shape)
        board = board[np.newaxis, :, :]
        # print(board.shape)
        pi, v = self.model.predict(board, verbose=False)

        return pi[0], v[0]
    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):
        # change extension
        filename = filename.split(".")[0] + ".h5"

        filepath = os.path.join(folder, filename)
        if not os.path.exists(folder):
            print("Checkpoint Directory does not exist! Making directory {}".format(folder))
            os.mkdir(folder)
        else:
            print("Checkpoint Directory exists! ")
        self.model.save_weights(filepath)

    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):
        # change extension
        filename = filename.split(".")[0] + ".h5"

        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98
        filepath = os.path.join(folder, filename)
        if not os.path.exists(filepath):
            raise("No model in path {}".format(filepath))

        self.model.load_weights(filepath)


def get_hash(s): 
    copy_s = copy.deepcopy(s)
    copy_s['walls'] = copy_s['walls'].tolist()
    return json.dumps(copy_s)

# def eval_reward(nnet, examples ): #does pit allow nns to use MCTS as well ? presumably yes. 

def assignRewards(examples): # to do: should reward be future reward or total reward achieved this episode? OpenAI says (at least for Policy optimization) to only consider future reward when evaluating consequences of actions. Imagine it's the same here.
    for i, ex in enumerate(examples):
        examples[i][2] = np.sum([ex[2] for ex in examples[i:]]) 
    return examples 

class MCTS(): 
    def __init__(self, env): 

        self.env = env

        self.c_puct = 100#variable controlling degree of exploration. 
        # self.visited = []
        self.P = {} # mapping from state -> distribution over policy starting at that state 
        self.N = {} 
        self.Q = {} 

    def search(self, s, nnet, cur_depth=0, depth_limit = None):
        if depth_limit != None and cur_depth >= depth_limit: 
            p, v = nnet.predict(self.env.observation(s))
            return v

        if self.env.done(s): 
            return self.env.reward(s) #if game is over. TODO: think about if this should be 0 to avoid doublecounting. 
        #else, game is not over, so we keep searching 
        sh = get_hash(s)

        if sh not in self.P:# if we aren't visited, use network to predict policy / value
            # self.visited.append(s)
            p, v = nnet.predict(self.env.observation(s))
            # print(p)
            # print(v)
            self.P[sh] = {self.env.actions[i] : p[i] for i in range(self.env.action_size)} #note that this can assign probability to invalid moves
            return v
        
        if sh not in self.N.keys():
            self.N[sh] = {a : 0 for a in self.env.get_valid_moves()}  
            self.Q[sh] = {a : 0 for a in self.env.get_valid_moves()} 
        #if we are visited, explore at this point, decide what the best action is to take according to upper confidence bound.  
        max_u, best_a = -float("inf"), -1
        for a in self.env.get_valid_moves(s):
            numerator = np.sum([self.N[sh][b] for b in self.env.get_valid_moves(s)])
            u = self.Q[sh][a] + self.c_puct*self.P[sh][a]*np.sqrt(numerator)/(1+self.N[sh][a] )
            if u>max_u:
                max_u = u
                best_a = a
        a = best_a #we've found the best action according to UCB
        
        sp = self.env.make_move(s, a)
        v = self.search(sp, nnet, cur_depth+1, depth_limit) + self.env.reward(sp) #we make the move and recurse, find the future value which is the current value + the value in all future timesteps


        self.Q[sh][a] = (self.N[sh][a] *self.Q[sh][a]  + v)/(self.N[sh][a]+1)
        self.N[sh][a]  += 1

        return v
    
    def pi(self, s): 
        sh = get_hash(s)
        if sh not in self.N: 
            print(f'key error: {sh}, using uniform')
            unif = []
            for a in self.env.get_valid_moves(): 
                if a in self.env.get_valid_moves(s): 
                    unif.append(1 / len(self.env.get_valid_moves(s)) ) 
                else: 
                    unif.append(0)
            return unif
        else: 
            # print(f'not a key error yay')
            denominator = np.sum([self.N[sh][a]  for a in self.env.get_valid_moves(s)])
            probs = []
            for a in self.env.get_valid_moves(): 
                if a in self.env.get_valid_moves(s): 
                    probs.append(self.N[sh][a] / denominator) 
                else: 
                    probs.append(0)
            return probs

def executeEpisode(env, nnet, numMCTSSims=10, depth_limit=3): #have the network try to play an environment to get some training data. 
    examples_episode = []
    s = env.init_state()
    mcts = MCTS(env)                                           # initialise search tree
        
    while True: #each run of this loop corresponds to one move in the game. 
        for _ in range(numMCTSSims):
            mcts.search(s, nnet, depth_limit=depth_limit)
        
        examples_episode.append([env.observation(s), mcts.pi(s), None])   # rewards can not be determined yet.  
        a = np.random.choice(range(env.action_size), p=mcts.pi(s))    # sample action from improved policy
        a = env.actions[a]
        s = env.make_move(s,a)
        examples_episode[-1][2] = env.reward(s) #assign the reward of the next state

        if env.done(s):
            return assignRewards(examples_episode) 

#Main Training Loop 
def policyIterSP(env, nnet, examples = [], numIters=10, numEps=50, numTest=10, training_run_name=''):
    curr_r = 0
    print(f'evaluating initial nnet...')
    S = [ env.init_state() for _ in tqdm(range(numTest))] #test state to evaluate each 
    r, r_s = evaluate_nnet(env, nnet, S) 

    for i in range(numIters):
        print(f'--- Iteration: {i} ---')
        print('gathering data by self play...')
        new_examples = copy.deepcopy(examples)
        for e in tqdm(range(numEps)):
            new_examples += executeEpisode(env, nnet, numMCTSSims=5, depth_limit=2)          # collect examples from this environment for which it played. 
        print(f'\ntraining on {len(new_examples)} examples...')
        new_nnet = copy.deepcopy(nnet)
        new_nnet.train(new_examples)
        new_nnet.save_checkpoint(folder=f'{training_run_name}_checkpoints', filename=f'iter_{i}_checkpoint.pth.tar')     

        print(f'evaluating new vs old nnet...')

        r_new, r_new_s = evaluate_nnet(env, new_nnet, S)
        if r_new > r: 
            if verbose > 0: print(f'Victory! r: {r:.4f}+-{r_s:.4f}, r_new: {r_new:.4f}+-{r_new_s:.4f}')
            r = r_new
            r_s = r_new_s
            nnet = new_nnet      
            examples = new_examples     
        else: 
            if verbose > 0: print(f'Defeat! r: {r:.4f}+-{r_s:.4f}, r_new: {r_new:.4f}+-{r_new_s:.4f}') 
    return nnet

### Evaluation code 

def evaluate_nnet_episode(env, nnet,s, numMCTSSims, depth_limit=None):
    if verbose > 2: print('episode start')
    mcts = MCTS(env)      
    r_sum = env.reward(s)                                     # initialise search tree
    while True: #each run of this loop corresponds to one move in the game. 
        for _ in range(numMCTSSims):
            mcts.search(s, nnet, depth_limit=depth_limit)
        a = np.random.choice(range(env.action_size), p=mcts.pi(s))    # sample action from improved policy
        a = env.actions[a]
        s = env.make_move(s,a)
        if verbose > 3: env.print_state(s)
        r = env.reward(s)
        r_sum += r
        if env.done(s):
            return r_sum

def evaluate_nnet(env, nnet, S, numMCTSSims=10, parallel=False): #have the network try to play an environment to get some training data. 
    if parallel: 
        process = lambda s : evaluate_nnet_episode(env, nnet,s, numMCTSSims)
        results = Parallel(n_jobs=20)(delayed(process)(s) for s in S)
    else: 
        results = [evaluate_nnet_episode(env, nnet,s, numMCTSSims) for s in S]


    return np.mean(results), np.std(results)

def main(): 
    env = Environment(size=10, total_moves=100, factor=1, random=True)
    nnet = NNet(env)                                       # initialise random neural network
    examples = []

    for e in tqdm(range(0)):
        examples += executeEpisode(env, nnet, numMCTSSims=10, depth_limit=None)

    policyIterSP(env, nnet, examples = examples, numIters=100,training_run_name='sep5evening')

if __name__ == '__main__': 
    main() 

